{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f9969b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dcf6975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading all text files\n",
    "location = \"C:\\\\Users\\\\ADMIN\\\\Desktop\\\\BlackCoffer\\\\Text_Files\\\\\"\n",
    "l = []\n",
    "for f in os.listdir(location):\n",
    "    l.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7adda066",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading stop words file\n",
    "with open('StopWords_Generic.txt', 'r', encoding='utf-8') as f:\n",
    "    stop_words = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2614ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = stop_words.split(\"\\n\")\n",
    "STOPWORDS = [x.lower() for x in STOPWORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82920a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Seq_num</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Word Proportion</th>\n",
       "      <th>Average Proportion</th>\n",
       "      <th>Std Dev</th>\n",
       "      <th>Doc Count</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Uncertainty</th>\n",
       "      <th>Litigious</th>\n",
       "      <th>Strong_Modal</th>\n",
       "      <th>Weak_Modal</th>\n",
       "      <th>Constraining</th>\n",
       "      <th>Syllables</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AARDVARK</td>\n",
       "      <td>1</td>\n",
       "      <td>354</td>\n",
       "      <td>1.550080e-08</td>\n",
       "      <td>1.422600e-08</td>\n",
       "      <td>3.815486e-06</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AARDVARKS</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1.313627e-10</td>\n",
       "      <td>8.653817e-12</td>\n",
       "      <td>9.241714e-09</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABACI</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>3.940882e-10</td>\n",
       "      <td>1.169679e-10</td>\n",
       "      <td>5.290465e-08</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABACK</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>1.269840e-09</td>\n",
       "      <td>6.654735e-10</td>\n",
       "      <td>1.595100e-07</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABACUS</td>\n",
       "      <td>5</td>\n",
       "      <td>8570</td>\n",
       "      <td>3.752595e-07</td>\n",
       "      <td>3.809464e-07</td>\n",
       "      <td>3.529356e-05</td>\n",
       "      <td>1108</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word  Seq_num  Word Count  Word Proportion  Average Proportion  \\\n",
       "0   AARDVARK        1         354     1.550080e-08        1.422600e-08   \n",
       "1  AARDVARKS        2           3     1.313627e-10        8.653817e-12   \n",
       "2      ABACI        3           9     3.940882e-10        1.169679e-10   \n",
       "3      ABACK        4          29     1.269840e-09        6.654735e-10   \n",
       "4     ABACUS        5        8570     3.752595e-07        3.809464e-07   \n",
       "\n",
       "        Std Dev  Doc Count  Negative  Positive  Uncertainty  Litigious  \\\n",
       "0  3.815486e-06         99         0         0            0          0   \n",
       "1  9.241714e-09          1         0         0            0          0   \n",
       "2  5.290465e-08          7         0         0            0          0   \n",
       "3  1.595100e-07         28         0         0            0          0   \n",
       "4  3.529356e-05       1108         0         0            0          0   \n",
       "\n",
       "   Strong_Modal  Weak_Modal  Constraining  Syllables     Source  \n",
       "0             0           0             0          2  12of12inf  \n",
       "1             0           0             0          2  12of12inf  \n",
       "2             0           0             0          3  12of12inf  \n",
       "3             0           0             0          2  12of12inf  \n",
       "4             0           0             0          3  12of12inf  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the master dictionary for positive and negative sentiments\n",
    "master = pd.read_csv('Loughran-McDonald_MasterDictionary_1993-2021.csv')\n",
    "master.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be8e4526",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing rows with missing values\n",
    "master.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61a4c45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making separate list for positive and negative words\n",
    "positive_words = list(master['Word'][master['Negative']==0])\n",
    "positive_words = [x.lower() for x in positive_words]\n",
    "negative_words = list(master['Word'][master['Negative']!=0])\n",
    "negative_words = [x.lower() for x in negative_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5f75a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03549693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_files(text_file):\n",
    "    with open(location + text_file,'r',encoding='utf-8') as f:\n",
    "        actual_text = f.read()\n",
    "    \n",
    "    return actual_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45169f8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4dc7df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removing_stopwords(original_text):\n",
    "    actual_text_list = original_text.split()\n",
    "        \n",
    "    text_without_stopwords = [word for word in actual_text_list if word.lower() not in STOPWORDS]\n",
    "    result = ' '.join(text_without_stopwords)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8fb81c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3d9f42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt_no_stopwords):\n",
    "    tokens = word_tokenize(txt_no_stopwords)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    #removing words that are less than 2 alphabets \n",
    "    tokens = [word for word in tokens if len(word) > 2]\n",
    "    # remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ce3295",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b72d27f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extracting_Derived_variables(text_tokenized):\n",
    "    pos_score = 0\n",
    "    neg_score = 0\n",
    "    \n",
    "    for i in range(len(text_tokenized)):\n",
    "        if text_tokenized[i] in positive_words:\n",
    "            pos_score+=1\n",
    "        elif text_tokenized[i] in negative_words:\n",
    "            neg_score-=1\n",
    "    \n",
    "    Pos_score = pos_score\n",
    "    #negative score is negative so multiplying it by -1 to make it a positive number\n",
    "    Neg_score = neg_score*(-1)\n",
    "    Polarity_Score = (Pos_score - Neg_score)/ ((Pos_score + Neg_score) + 0.000001)\n",
    "    Word_Count = len(text_tokenized)\n",
    "    Subjectivity_Score = (Pos_score + Neg_score)/((Word_Count) + 0.000001)\n",
    "    \n",
    "    return Pos_score , Neg_score, Polarity_Score, Subjectivity_Score, Word_Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb07bba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72b2fb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllable_count(word):\n",
    "    word = word.lower()\n",
    "    count = 0\n",
    "    vowels = \"aeiouy\"\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith(\"e\") and (word.endswith(\"able\")==False):\n",
    "        count -= 1\n",
    "    if (word.endswith(\"ed\") and (word.endswith(\"eed\")==False)) or (word.endswith(\"es\") and (word.endswith(\"ees\")==False)):\n",
    "        count-=1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "        \n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3314fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1cee87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_word_count(words_list):\n",
    "    complex_words=[]\n",
    "    for i in words_list:\n",
    "        if syllable_count(i) > 2:\n",
    "            complex_words.append(i)\n",
    "    return complex_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088926ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ad9c1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Analysis_of_Readability(original_text):\n",
    "    number_of_sentences = sent_tokenize(original_text)\n",
    "    Average_Sentence_Length = word_count /len(number_of_sentences)\n",
    "    \n",
    "    Percentage_of_Complex_words = len(complex_words) / word_count\n",
    "    \n",
    "    Fog_Index = 0.4 * (Average_Sentence_Length + Percentage_of_Complex_words)\n",
    "    \n",
    "    return Average_Sentence_Length,Percentage_of_Complex_words, Fog_Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca95d0f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae6f42bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Personal_Pronouns(original_text):\n",
    "    pronounRegex = re.compile(r'\\b(I|we|my|ours|(?-i:us))\\b',re.I)\n",
    "    pronouns = pronounRegex.findall(original_text)\n",
    "    \n",
    "    return len(pronouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21a0ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b459c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Average_Word_Length(words_list):\n",
    "    average = sum(len(word) for word in words_list) / word_count\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4157d910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9006910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_ID = []\n",
    "POSITIVE_SCORE = []              \n",
    "NEGATIVE_SCORE = []                     \n",
    "POLARITY_SCORE = []                \n",
    "SUBJECTIVITY_SCORE = []\n",
    "AVG_SENTENCE_LENGTH = []\n",
    "PERCENTAGE_OF_COMPLEX_WORDS =[]\n",
    "FOG_INDEX =[]\n",
    "AVG_NUMBER_OF_WORDS_PER_SENTENCE =[]\n",
    "COMPLEX_WORD_COUNT =[]\n",
    "WORD_COUNT = []\n",
    "SYLLABLE_PER_WORD =[]\n",
    "PERSONAL_PRONOUNS =[]\n",
    "AVG_WORD_LENGTH =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c25816c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(l)):\n",
    "    text = load_text_files(l[i])\n",
    "    text_no_stopwords = removing_stopwords(text)\n",
    "    cleaned_word_list = clean_text(text_no_stopwords)\n",
    "    positive_score , negative_score, polarity_score, subjectivity_score, word_count = Extracting_Derived_variables(cleaned_word_list)\n",
    "    syllable_list = list(map(syllable_count, cleaned_word_list))\n",
    "    syllable_per_word = sum(syllable_list)/word_count\n",
    "    complex_words = complex_word_count(cleaned_word_list)\n",
    "    Average_Sentence_Length,Percentage_of_Complex_words, Fog_Index = Analysis_of_Readability(text)\n",
    "    personal_pronouns = Personal_Pronouns(text)\n",
    "    average_word_length = Average_Word_Length(cleaned_word_list)\n",
    "    URL_ID.append(l[i].strip(\".txt\"))\n",
    "    POSITIVE_SCORE.append(positive_score)\n",
    "    NEGATIVE_SCORE.append(negative_score)                     \n",
    "    POLARITY_SCORE.append(round(polarity_score,2))             \n",
    "    SUBJECTIVITY_SCORE.append(round(subjectivity_score,2))\n",
    "    AVG_SENTENCE_LENGTH.append(round(Average_Sentence_Length,2))\n",
    "    PERCENTAGE_OF_COMPLEX_WORDS.append(round(Percentage_of_Complex_words*100, 2))\n",
    "    FOG_INDEX.append(round(Fog_Index,2))\n",
    "    AVG_NUMBER_OF_WORDS_PER_SENTENCE.append(round(Average_Sentence_Length,2))\n",
    "    COMPLEX_WORD_COUNT.append(len(complex_words))\n",
    "    WORD_COUNT.append(word_count)\n",
    "    SYLLABLE_PER_WORD.append(round(syllable_per_word, 2))\n",
    "    PERSONAL_PRONOUNS.append(personal_pronouns)\n",
    "    AVG_WORD_LENGTH.append(round(average_word_length,2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b02eab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'URL_ID':URL_ID,'POSITIVE SCORE':POSITIVE_SCORE, 'NEGATIVE SCORE':NEGATIVE_SCORE, 'POLARITY SCORE':POLARITY_SCORE,\\\n",
    "    'SUBJECTIVITY SCORE':SUBJECTIVITY_SCORE, \"AVG SENTENCE LENGTH\": AVG_SENTENCE_LENGTH, 'PERCENTAGE OF COMPLEX WORDS':PERCENTAGE_OF_COMPLEX_WORDS,\\\n",
    "    'FOG INDEX':FOG_INDEX, 'AVG NUMBER OF WORDS PER SENTENCE':AVG_NUMBER_OF_WORDS_PER_SENTENCE, 'COMPLEX WORD COUNT':COMPLEX_WORD_COUNT,\\\n",
    "    'WORD COUNT':WORD_COUNT,'SYLLABLE PER WORD':SYLLABLE_PER_WORD, 'PERSONAL PRONOUNS':PERSONAL_PRONOUNS, 'AVG WORD LENGTH':AVG_WORD_LENGTH }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0698a36d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>95</td>\n",
       "      <td>654</td>\n",
       "      <td>44</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.93</td>\n",
       "      <td>11.06</td>\n",
       "      <td>21.81</td>\n",
       "      <td>4.51</td>\n",
       "      <td>11.06</td>\n",
       "      <td>164</td>\n",
       "      <td>752</td>\n",
       "      <td>1.92</td>\n",
       "      <td>4</td>\n",
       "      <td>6.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>96</td>\n",
       "      <td>703</td>\n",
       "      <td>40</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.88</td>\n",
       "      <td>13.90</td>\n",
       "      <td>30.54</td>\n",
       "      <td>5.68</td>\n",
       "      <td>13.90</td>\n",
       "      <td>259</td>\n",
       "      <td>848</td>\n",
       "      <td>2.17</td>\n",
       "      <td>3</td>\n",
       "      <td>6.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>97</td>\n",
       "      <td>386</td>\n",
       "      <td>22</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.92</td>\n",
       "      <td>40.45</td>\n",
       "      <td>13.71</td>\n",
       "      <td>16.24</td>\n",
       "      <td>40.45</td>\n",
       "      <td>61</td>\n",
       "      <td>445</td>\n",
       "      <td>1.62</td>\n",
       "      <td>46</td>\n",
       "      <td>5.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>98</td>\n",
       "      <td>836</td>\n",
       "      <td>61</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.93</td>\n",
       "      <td>15.32</td>\n",
       "      <td>29.53</td>\n",
       "      <td>6.25</td>\n",
       "      <td>15.32</td>\n",
       "      <td>285</td>\n",
       "      <td>965</td>\n",
       "      <td>2.07</td>\n",
       "      <td>10</td>\n",
       "      <td>6.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>99</td>\n",
       "      <td>550</td>\n",
       "      <td>16</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.98</td>\n",
       "      <td>7.57</td>\n",
       "      <td>20.52</td>\n",
       "      <td>3.11</td>\n",
       "      <td>7.57</td>\n",
       "      <td>118</td>\n",
       "      <td>575</td>\n",
       "      <td>1.86</td>\n",
       "      <td>7</td>\n",
       "      <td>6.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    URL_ID  POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  \\\n",
       "165     95             654              44            0.87   \n",
       "166     96             703              40            0.89   \n",
       "167     97             386              22            0.89   \n",
       "168     98             836              61            0.86   \n",
       "169     99             550              16            0.94   \n",
       "\n",
       "     SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  \\\n",
       "165                0.93                11.06                        21.81   \n",
       "166                0.88                13.90                        30.54   \n",
       "167                0.92                40.45                        13.71   \n",
       "168                0.93                15.32                        29.53   \n",
       "169                0.98                 7.57                        20.52   \n",
       "\n",
       "     FOG INDEX  AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  \\\n",
       "165       4.51                             11.06                 164   \n",
       "166       5.68                             13.90                 259   \n",
       "167      16.24                             40.45                  61   \n",
       "168       6.25                             15.32                 285   \n",
       "169       3.11                              7.57                 118   \n",
       "\n",
       "     WORD COUNT  SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "165         752               1.92                  4             6.49  \n",
       "166         848               2.17                  3             6.88  \n",
       "167         445               1.62                 46             5.49  \n",
       "168         965               2.07                 10             6.96  \n",
       "169         575               1.86                  7             6.19  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(d)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "511a69be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_urls = pd.read_excel(\"Input.xlsx\")\n",
    "df_urls['URL_ID'] = df_urls['URL_ID'].astype(str)\n",
    "df_urls['URL_ID']  = df_urls['URL_ID'].apply(lambda x : x.rstrip('0').rstrip('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f89b9458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://insights.blackcoffer.com/how-is-login-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://insights.blackcoffer.com/how-does-ai-h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-its-im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://insights.blackcoffer.com/how-do-deep-l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://insights.blackcoffer.com/how-artificia...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  URL_ID                                                URL\n",
       "0      1  https://insights.blackcoffer.com/how-is-login-...\n",
       "1      2  https://insights.blackcoffer.com/how-does-ai-h...\n",
       "2      3  https://insights.blackcoffer.com/ai-and-its-im...\n",
       "3      4  https://insights.blackcoffer.com/how-do-deep-l...\n",
       "4      5  https://insights.blackcoffer.com/how-artificia..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_urls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd16e346",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = df_urls.merge(df, on='URL_ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "090d12ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(\"Output Data Structure.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee4b0553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://insights.blackcoffer.com/how-is-login-...</td>\n",
       "      <td>386</td>\n",
       "      <td>5</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.96</td>\n",
       "      <td>16.92</td>\n",
       "      <td>23.89</td>\n",
       "      <td>6.86</td>\n",
       "      <td>16.92</td>\n",
       "      <td>97</td>\n",
       "      <td>406</td>\n",
       "      <td>1.99</td>\n",
       "      <td>4</td>\n",
       "      <td>6.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://insights.blackcoffer.com/how-does-ai-h...</td>\n",
       "      <td>369</td>\n",
       "      <td>6</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.98</td>\n",
       "      <td>13.64</td>\n",
       "      <td>29.58</td>\n",
       "      <td>5.58</td>\n",
       "      <td>13.64</td>\n",
       "      <td>113</td>\n",
       "      <td>382</td>\n",
       "      <td>2.13</td>\n",
       "      <td>2</td>\n",
       "      <td>6.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-its-im...</td>\n",
       "      <td>924</td>\n",
       "      <td>23</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>13.36</td>\n",
       "      <td>34.23</td>\n",
       "      <td>5.48</td>\n",
       "      <td>13.36</td>\n",
       "      <td>343</td>\n",
       "      <td>1002</td>\n",
       "      <td>2.22</td>\n",
       "      <td>13</td>\n",
       "      <td>7.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://insights.blackcoffer.com/how-do-deep-l...</td>\n",
       "      <td>255</td>\n",
       "      <td>1</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "      <td>17.33</td>\n",
       "      <td>30.77</td>\n",
       "      <td>7.06</td>\n",
       "      <td>17.33</td>\n",
       "      <td>80</td>\n",
       "      <td>260</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1</td>\n",
       "      <td>7.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://insights.blackcoffer.com/how-artificia...</td>\n",
       "      <td>355</td>\n",
       "      <td>12</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.96</td>\n",
       "      <td>10.11</td>\n",
       "      <td>34.38</td>\n",
       "      <td>4.18</td>\n",
       "      <td>10.11</td>\n",
       "      <td>132</td>\n",
       "      <td>384</td>\n",
       "      <td>2.23</td>\n",
       "      <td>21</td>\n",
       "      <td>7.07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  URL_ID                                                URL  POSITIVE SCORE  \\\n",
       "0      1  https://insights.blackcoffer.com/how-is-login-...             386   \n",
       "1      2  https://insights.blackcoffer.com/how-does-ai-h...             369   \n",
       "2      3  https://insights.blackcoffer.com/ai-and-its-im...             924   \n",
       "3      4  https://insights.blackcoffer.com/how-do-deep-l...             255   \n",
       "4      5  https://insights.blackcoffer.com/how-artificia...             355   \n",
       "\n",
       "   NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  \\\n",
       "0               5            0.97                0.96                16.92   \n",
       "1               6            0.97                0.98                13.64   \n",
       "2              23            0.95                0.95                13.36   \n",
       "3               1            0.99                0.98                17.33   \n",
       "4              12            0.93                0.96                10.11   \n",
       "\n",
       "   PERCENTAGE OF COMPLEX WORDS  FOG INDEX  AVG NUMBER OF WORDS PER SENTENCE  \\\n",
       "0                        23.89       6.86                             16.92   \n",
       "1                        29.58       5.58                             13.64   \n",
       "2                        34.23       5.48                             13.36   \n",
       "3                        30.77       7.06                             17.33   \n",
       "4                        34.38       4.18                             10.11   \n",
       "\n",
       "   COMPLEX WORD COUNT  WORD COUNT  SYLLABLE PER WORD  PERSONAL PRONOUNS  \\\n",
       "0                  97         406               1.99                  4   \n",
       "1                 113         382               2.13                  2   \n",
       "2                 343        1002               2.22                 13   \n",
       "3                  80         260               2.17                  1   \n",
       "4                 132         384               2.23                 21   \n",
       "\n",
       "   AVG WORD LENGTH  \n",
       "0             6.67  \n",
       "1             6.91  \n",
       "2             7.19  \n",
       "3             7.02  \n",
       "4             7.07  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6636c4d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
